# Bigram LLM Pre-Training

## Overview

This project implements a Bigram Language Model (LLM) for pre-training on a given dataset. The model is designed to understand and generate text based on the statistical properties of bigrams.

## Features

- **Bigram Model**: Utilizes bigram statistics for language modeling.
- **Pre-Training**: Pre-trains the model on a specified dataset.
- **Flexible Architecture**: Easily adjustable parameters for experimentation.

## Requirements

- Python 3.x
- PyTorch
- Other dependencies (list any additional libraries)

## Installation

1. git clone https://github.com/arnab-mkj/Bigram_LLM_pre_training.git
1. cd Bigram_LLM_pre_training


## Usage

To pre-train the Bigram LLM, run the following command:

```
python training.py -batch_size <batch size>
```


## Contributing

Contributions are welcome! Please open an issue or submit a pull request for any improvements or bug fixes.


## Acknowledgments

- [PyTorch](https://pytorch.org/) for the deep learning framework.
- [Your Dataset Source](https://huggingface.co/datasets/Skylion007/openwebtext) for providing the dataset used in this project.

<br>